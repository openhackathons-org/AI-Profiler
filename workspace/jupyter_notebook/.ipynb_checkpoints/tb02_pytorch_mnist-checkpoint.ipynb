{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"tb01_introduction.ipynb\">1</a>\n",
    "        <a>2</a>\n",
    "        <a href=\"tb03_data_transfer.ipynb\">3</a>\n",
    "        <a href=\"tb04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"tb03_data_transfer.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Optimizing PyTorch's MNIST Training Program\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is focused on optimizing a deep neural network (DNN) training program using the Modified National Institute of Standards and Technology (MNIST) dataset. \n",
    "\n",
    "\n",
    "## Running the Application\n",
    "\n",
    "The MNIST database consists of normalized fixed-size handwritten digits images. The database includes 60k training examples and 10k test examples. Click [here](http://yann.lecun.com/exdb/mnist/) to learn more about `LeCun et al., 1998` dataset. In this lab, the MNIST database will be used for training a DNN that recognizes handwritten digits. Our training program is adopted from [PyTorch GitHub] (https://github.com/pytorch/examples/tree/master/mnist) and written using PyTorch framework. The image below is an example of normalized digits from the testing set.\n",
    "\n",
    "<center><img src=\"images/mnist.jpg\"></center>\n",
    "<center><a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf\"> View source here<a/> </center>\n",
    "    \n",
    "Run the cell below to execute the baseline training program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../source_code && time python3 main_baseline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output on the NVIDIA® DGX™ A100**:\n",
    "\n",
    "```python\n",
    "\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308954\n",
    "\n",
    "Test set: Average loss: 0.1049, Accuracy: 9668/10000 (97%)\n",
    "\n",
    "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.166372\n",
    "\n",
    "Test set: Average loss: 0.0629, Accuracy: 9805/10000 (98%)\n",
    "\n",
    "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.116849\n",
    "\n",
    "Test set: Average loss: 0.0555, Accuracy: 9826/10000 (98%)\n",
    "\n",
    "---------------------------------------------------------\n",
    "\n",
    "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.057983\n",
    "\n",
    "Test set: Average loss: 0.0373, Accuracy: 9862/10000 (99%)\n",
    "\n",
    "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.055643\n",
    "\n",
    "Test set: Average loss: 0.0368, Accuracy: 9869/10000 (99%)\n",
    "\n",
    "\n",
    "real\t1m53.085s\n",
    "user\t2m20.216s\n",
    "sys\t 0m6.212s\n",
    "\n",
    "```\n",
    "It takes approximately`2 minutes` to execute the `10 epochs` in the training program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile the Application\n",
    "\n",
    "Remember in Part 1 of the lab we gave a summary of the train function in [main_baseline.py](../source_code/main_baseline.py) as: \n",
    "- Data is copied from CPU to the GPU (device),\n",
    "- Forward pass runs on the GPU, and\n",
    "- Backward pass runs on the GPU.\n",
    "\n",
    "The first step in our optimization workflow is to use the PyTorch Profiler command to wrap the `profiler schedule` and the `tensorboard_trace_handler` and create the Profiler object `prof`. The Profiler object should be placed in the section of our application code to be profile by calling the `prof.start()` and `prof.stop()` methods. You can see the preview below in `green frame` within our PyTorch mnist `main` method screenshot.\n",
    "  \n",
    "\n",
    "\n",
    "```python\n",
    "import torch.profiler as profiler\n",
    "\n",
    ".................................\n",
    "\n",
    "prof = profiler.profile( schedule=profiler.schedule( wait=1, warmup=1, active=3, repeat=2),\n",
    "                         on_trace_ready=profiler.tensorboard_trace_handler('../log/mnist'),\n",
    "                         record_shapes=True,with_stack=True)\n",
    "prof.start()\n",
    ".............................\n",
    "# code to profile\n",
    "............................\n",
    "\n",
    "prof.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/tb_main.jpg width=70%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly review the profiling commands used.\n",
    "\n",
    "- `schedule=profiler.schedule ( wait=1, warmup=1, active=3, repeat=2)`: command to schedule when to start profiling. This command is already explained in the previous notebook. The Profiler skips the first step, start warming up on the next step, and start profiling trace for the next three steps. The `repeat=2` implies repeating the profiling cycle twice. \n",
    "-`on_trace_ready=profiler.tensorboard_trace_handler('../log/mnist')`: handles all the profile trace from `profiler.schedule` and saves them in the `../log/mnist` for visualization on the TensorBoard \n",
    "- `record_shapes=True`: command to record shapes of operator’s input\n",
    "- `with_stack=True`: command to record source information for the ops. This is responsible for file and line number record \n",
    "- `prof.start()`: start profile\n",
    "- `prof.stop()`: stop profile \n",
    "\n",
    "\n",
    "Our focus is on the training process, therefore the profiler object `prof` is passed unto the train method to capture profile trace at specified active steps `prof.step()` in the `profiler.schedule`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/tb_train.png width=60%>\n",
    "Please run the cell below to profile the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ../source_code/tb_main_baseline_profiler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling, run the cell below to visualize the profile trace in the `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=../log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to view the visualization. First, if you are running this on your local machine, open the Google Chrome browser and type `localhost:6006/`\n",
    "\n",
    "<img src=images/browse_port.jpg width=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, if you are running on a remote machine, for example on the DGX A100, you have to do `port forwarding` on port `6006` as shown in the screenshot below. Next, open the Google Chrome browser and type `localhost:6006/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/port_forwarding.jpg width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyze the profile using the TensorBoard visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview is shown below.\n",
    "\n",
    "<img src=images/profile_summary.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the GPU Summary frame, the `GPU utilization` is under 8% which is very low. We can also see the same in the `Execution Summary` panel looking at the `kernel` row. The most noticeable one is the `DataLoader` which took ~89% of the time. In the `Step Time Breakdown` panel below, the `DataLoader` is seen to be consuming most step time (in microseconds).\n",
    "\n",
    "<img src=images/step_time_breakdown.jpg>\n",
    "<img src=images/performance_recommendation.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Performance Recommendation` panel suggests setting the number of workers `(num_workers)` on DataLoader and enable multi-processing data loading. There are two steps to do this:\n",
    "- Increase the value of `num_workers` (if already set)\n",
    "- Enable `pinned memory` because it's a CPU bottleneck issue from `memory pageable` (Data Transfers between Host and Device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Code to Address the CPU Bottleneck\n",
    "Let's inspect the data loader [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) used in our application. From the code in `tb_main_baseline_nvtx.py` shown below, a single worker subprocess is asynchronously loading the data.\n",
    "\n",
    "<img src=images/NumberOfWorkers.jpg width=50%>\n",
    "\n",
    "To increase the overlap between data loading and training on the GPU, the `num_workers` parameter should be increased. Run the following cell to see the code changes made to tune this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!diff -d --color=always ../source_code/tb_main_baseline_profiler.py ../source_code/tb_main_opt1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of CPU cores available on the target system, we can increase `num_of_workers` to  use the total number of CPU cores available by setting the `num_workers` to `multiprocessing.cpu_count()`, `2`, and `more`, to improve the overlap. There are rare situations where setting `num_workers` to `multiprocessing.cpu_count()` would prompt the following warnings:\n",
    "\n",
    "```python\n",
    "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 256 worker processes in total. \n",
    "Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. \n",
    "Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
    "```\n",
    "\n",
    "Then, you may have to adjust to the recommended maximum number of workers. Let's increase the number of workers to 2 `'num_workers': 2`. \n",
    "\n",
    "## Profile to Verify Optimization\n",
    "Profile again by executing the cell below to verify if the code change addresses the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ../source_code/tb_main_opt1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling, run the cell below to visualize the profile trace in the `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=../log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a remote machine, remember to do `port-forwarding` as described above before opening the browser at `localhost:6006/`.\n",
    "\n",
    "<img src=images/profile_summary_opt11.jpg>\n",
    "<img src=images/trace_view_opt11.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see the `GPU utilization` increases from `~8%` to `38%` on the `GPU Summary` and `Trace view ` panels. Meanwhile the `DataLoader` running time on the CPU has reduced from `~89%` to `48.34%` as shown in the `Execution Summary` panel. The `Step Time Breakdown` likewise reflects these changes, especially in the third and fourth steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/step_time_breakdown_opt11.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to the next notebook to implement the second proposed memory optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA Nsight™ Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download the latest version of NVIDIA Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "\n",
    "You can also get resources from [Open Hackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"tb01_introduction.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"tb03_data_transfer.ipynb\">3</a>\n",
    "        <a href=\"tb04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"tb03_data_transfer.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
