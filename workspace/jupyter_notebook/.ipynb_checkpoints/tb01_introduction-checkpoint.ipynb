{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a >1</a>\n",
    "        <a href=\"tb02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a href=\"\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"tb02_pytorch_mnist.ipynb\">Next Notebook</a></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Using PyTorch Profiler with TensorBoard plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is an overview of the PyTorch Profiler. The goal is to familiarize you with commonly used features of the TensorBoard visualization toolkit. To get started, we will recap what profiling is from the Part 1 section of this lab.\n",
    "\n",
    "## What is profiling?\n",
    "\n",
    "Profiling is the first step in optimizing and tuning your application. Profiling an application helps to understand where most of the execution time is spent, providing an understanding of performance characteristics and identifying parts of the code that present opportunities for improvement. Finding hotspots and bottlenecks in your application can help you decide where to focus your optimization efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Profiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch Profiler tool enables the profiling of deep neural networks (DNN) training programs through the collection of performance metrics that include execution time, memory costs, stack traces, device Kernel, and more. This process is done through the context manager application processing interface (API). The PyTorch Profiler API assists in identifying the most expensive operators such as hot spots within the application. The Profiler also supports multithreaded models.\n",
    "\n",
    "To be able to profile the application using PyTorch Profiler, you must import essential libraries as shown below: \n",
    "\n",
    "```python\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.profiler import schedule\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can achieve the following tasks with the PyTorch profiler:\n",
    "\n",
    "- **Analyze Execution Time**\n",
    "\n",
    "The context manager activates the PyTorch Profiler to measure the execution time on the Host `(CPU)` and the device Kernel `(CUDA)` activities using `ProfilerActivity`. It also records the shapes of the operator inputs and reports the amount of memory consumed by the model’s Tensors. Below are the lines of code needed to achieve this.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "with profile(activities=[ ProfilerActivity.CUDA, ProfilerActivity.CPU], \n",
    "             record_shapes=True) as p:\n",
    "    with record_function(\"running_model\"):\n",
    "        model(data)\n",
    "```\n",
    "\n",
    "\n",
    "- **Analyze Memory Consumption and Examine Stack Traces**\n",
    "\n",
    "\n",
    "The PyTorch Profiler reveals the amount of memory consumed by the model’s tensors or released during the execution of the model’s operators. In addition, it gives an analysis of the stack traces. To capture these, you must include `profile_memory=True` and `with_stack=True` in the `profile` as shown below:\n",
    "\n",
    "```python\n",
    "with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], \n",
    "             record_shapes=True, profile_memory=True, with_stack=True) as p:\n",
    "    model(data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Analyze Long-running Jobs**\n",
    "\n",
    "Long-running jobs are referred to as model training that runs within loops (step or batch iterations). In this case, the profiling process can take a long time and result in a heavy trace file being generated. Therefore, PyTorch Profiler offers `torch.profiler.schedule` API to schedule the number of steps to trace execution and return profiling results.\n",
    "\n",
    "```python\n",
    "schedule = schedule(skip_first=5, wait=1, warmup=1, active=3)\n",
    "```\n",
    "In the above statement, `skip_first=5` implies the profiler should ignore the first 5 steps, `wait=1` means the profiler is to be idle for 1 step, `warmup=1` depicts starts tracing using 1 step as for warming up but discard the result due to overheads incurred at the beginning of profiling trace, `active=3` tells the profiler to start the trace and collect performance metrics. We can combine the schedule with the profiling statement as given below:\n",
    "\n",
    "```python\n",
    "with profile( activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], \n",
    "             schedule=torch.profiler.schedule(wait=1, warmup=1,active=2), \n",
    "             on_trace_ready=trace_handler) as p:\n",
    "    for step, data in enumerate(train_loader):\n",
    "        train(data)\n",
    "        p.step() \n",
    "```\n",
    "For more information please visit the [PyTorch Profiler page](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorBoard is a TensorFlow visualization toolkit that provides the tooling needed for tracking and visualizing performance metrics in a machine learning workflow. Through the TensorBoard plugin, the PyTorch Profiler trace can be visualized and analyzed on the TensorBoard. TensorBoard visualization can be viewed through `http://localhost:6006/#pytorch_profiler` URL in the Google Chrome or Microsoft Edge browser.\n",
    "\n",
    "<img src=images/browse_port.jpg width=90%>\n",
    "\n",
    "Further details will be provided in the next notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Highlight of TensorBoard visualization features\n",
    "\n",
    "The main features include:\n",
    "\n",
    "- Overview\n",
    "- Operator\n",
    "- GPU Kernel\n",
    "- Trace\n",
    "- Module\n",
    "\n",
    "<img src=images/side_menu_list.jpg width=20%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Overview**\n",
    "\n",
    "The overview view gives a summary of training model performance metrics that include `Configuration`, `GPU Summary`, `Execution Summary`, `Step Time Breakdown`, and `Performance Recommendation`. In the configuration panel, you can see the `Number of Worker(s)` and the `Device Type` that indicates a device such as a GPU. GPU features like GPU index and name, compute capability, utilization, estimated shared memory, achieved occupancy, and kernel time using Tensor Cores are found on the `GPU Summary` panel. The `Execution Summary` displays a statistical summary of time duration and percentage of use for important processes like Kernel, Memcpy, Memset, Runtime, CPU execution, and others. Meanwhile, the `Step Time Breakdown` represents the processes in the `Execution Summary` panel as a step time in microseconds (μs) stacked bar graph. In the `Performance Recommendation` panel, you will find suggestions that help identify bottlenecks as well as steps to solving and improving the performance of the profiled code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/tb_overview.png width=100%>\n",
    "<center> <a href=\"https://pytorch.org/tutorials/_static/img/profiler_overview1.png\">view source here</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Operator**\n",
    "\n",
    "The `Operator` view provides a graphical representation of the performance of PyTorch operators executed on the CPU and GPU. The percentage statistics on the host and device are shown below in μs. At the bottom of the “Operator” view, you can see in a table the statistics displayed in the graphical form above. At the right-end column of the table, you will see the `View CallStack` that displays the call frames link `View call frames`. This provides links to line numbers that trigger the operations of PyTorch operators within your code. Clicking on the stack calls line numbers launches VSCode (if installed). The section of the code is displayed otherwise, you have to manually open the code and search for the line number within your preferred IDE (Integrated Development Environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/operator_view.jpg width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/operator_view_tbl.jpg width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/operator_view_code_trace.jpg width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GPU Kernel**\n",
    "\n",
    "In the `Views` selection dropdown from the `NORMAL` panel on the right side of the visualization, select `GPU Kernel` to display the “Kernel View.” This displays the percentage statistics of all kernels launched in your training model. More importantly, it shows the percentage statistics of Tensor Cores used and not used. At the bottom of the graphical chart is the table that lists kernels running within the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/tb_kernel_view.png width=100%>\n",
    "<center><a href=\"https://pytorch.org/tutorials/_static/img/profiler_kernel_view.png\">View source here</a>  </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Trace**\n",
    "\n",
    "The `Trace` view shows a timeline of profiled operators including the code line number for the threads. `GPU Utilization and SM Efficiency estimation` can also be seen on the `Trace` view timeline. You can zoom in/out the timeline using the arrow toolbar at the right-side of the timeline view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/trace_view1.jpg width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Module**\n",
    "\n",
    "The `Module` view shows your application summary, displaying the name of the module/class running within your application model, the number of `occurrences` (the number of times it was called) and `operators`, and the total amount of time spent on the CPU and GPU.\n",
    "\n",
    "<img src=images/module_view.jpg width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to proceed to start profiling a simple DNN training program. In the rest of the notebooks, the expression `DNN training program` will be used interchangeably with the word 'application'. Please, click on the `Next Notebook` link at the bottom to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA Nsight™ Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download the latest version of NVIDIA Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "\n",
    "You can also get resources from [Open Hackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    "\n",
    "## References\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/profiler.html\n",
    "- https://pytorch.org/docs/stable/profiler.html\n",
    "- https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "- https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html\n",
    "- https://www.tensorflow.org/tensorboard/get_started\n",
    "---\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a >1</a>\n",
    "        <a href=\"tb02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a href=\"04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"tb02_pytorch_mnist.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
