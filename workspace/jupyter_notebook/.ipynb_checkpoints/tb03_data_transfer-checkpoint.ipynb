{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"tb01_introduction.ipynb\">1</a>\n",
    "        <a href=\"tb02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a>3</a>\n",
    "        <a href=\"tb04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"tb04_tensor_core_util.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Memory Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to optimize memory operations that reflect data transfers between Host and Device. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Previous Profile Trace \n",
    "Let's analyze the data transfers from the host and device from the first optimization step in the previous notebook. In the screenshot below, you can see the time taken by `Memcpy`. The first goal is to reduce the time, but before that can happen we need to investigate what is happening on the device's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/profile_summary_opt11_memcpy.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the memory view, you must ensure that `profile_memory=True` is set in the PyTorch Profiler. `The Memory View` records and displays all memory allocations and reserves from the GPU & CPU in a curve graph. The view also shows the memory events and statistics in tabular form. The first step is to look by the left-side of the `TensorBoard` under `NORMAL`, you will see the “Views” dropdown selection box. Please select `Memory` as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/memory_view_menu.jpg\" height=\"30%\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interest is GPU memory usage therefore, select GPU under `Device` at the top left corner of the `Memory View` panel. In the memory curve graph screenshot below, we see almost zero memory usage on the time axis from 8ms to 35ms. This implies that a small amount of GPU memory is being utilized therefore `memcpy` will take longer and affect data transfer. The remedy would be to enable `Pinned Memory` to utilize more GPU memory and foster faster data transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/memory_view_opt11.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Pinned Memory\n",
    "\n",
    "Host (CPU) memory allocations are pageable by default. The GPU cannot access data directly from pageable host memory. When a data transfer is invoked from pageable host memory to device memory, the NVIDIA® CUDA® driver must first allocate a temporary page-locked (or “pinned”) host array, copy the host data to the pinned array, and then transfer the data from the pinned array to the device memory. The pinned memory is used as a staging area for transfers from the host to the device. By directly allocating our host data to pinned memory, we can avoid this extra step and its overhead. See the following blog [post](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/) for more details.\n",
    "\n",
    "<img src=images/PageableVsPinned.jpg width=50%> \n",
    "<b>Source:</b> <i> https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc </i><br><br/>\n",
    "\n",
    "We add `pin_memory: True` to the settings used for the data loader in our program. Run the cell below to see the code change (shown in green) made to use pinned memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--- ../source_code/main_opt1.py\t2022-08-18 03:51:54.178624632 +0900\u001b[0m\n",
      "\u001b[1m+++ ../source_code/main_opt2.py\t2022-08-19 23:25:01.750840799 +0900\u001b[0m\n",
      "\u001b[36m@@ -160,8 +160,9 @@\u001b[0m\n",
      "     test_kwargs = {'batch_size': args.test_batch_size}\n",
      "     if use_cuda:\n",
      "         #multiprocessing.cpu_count()\n",
      "         cuda_kwargs = {'num_workers': 2,\n",
      "\u001b[32m+                       'pin_memory': True,\u001b[0m\n",
      "                        'shuffle': True}\n",
      "         train_kwargs.update(cuda_kwargs)\n",
      "         test_kwargs.update(cuda_kwargs)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "!diff -U4 --color=always ../source_code/tb_main_opt1.py ../source_code/tb_main_opt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Again to Verify Optimization\n",
    "Profile again by executing the cell given below to verify if our code change addresses the problem with host-to-device memory transfers after setting `pin_memory: True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ../source_code/tb_main_opt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with the profiling, run the cell below to visualize the profile trace in the `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=../log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on a remote machine, remember to do `port-forward` as described in the previous notebook before opening the browser at `localhost:6006/` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/profile_summary_opt22.jpg\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/memory_view_opt22.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are noticeable changes on the TensorBoard: \n",
    "- In the `Memory View` we can see the memory usage has increased as compared to the way it was before.\n",
    "- On the `Execution Summary` panel, the `memcpy` has dropped from `346µs` to `173µs`\n",
    "- Also, the time taken by the `DataLoader` has reduced to `4,4638µs (~29%)` as compared to `7,770µs (48.3%)` from the previous notebook.\n",
    "\n",
    "Now that we have addressed a bottleneck with memory transfers, let's identify the next performance bottleneck by clicking on the `Next Notebook` link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Links and Resources\n",
    "\n",
    "[NVIDIA Nsight™ Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download the latest version of NVIDIA Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "You can also get resources from [openhackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    " \n",
    " ---\n",
    " ## Licensing\n",
    "  \n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"tb01_introduction.ipynb\">1</a>\n",
    "        <a href=\"tb02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a >3</a>\n",
    "        <a href=\"tb04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"tb04_tensor_core_util.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
