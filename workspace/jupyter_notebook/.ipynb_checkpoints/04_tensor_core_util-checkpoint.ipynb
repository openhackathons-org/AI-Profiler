{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"05_summary.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Tensor Cores \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show how to enable mixed precision (FP32/FP16) on the Tensor Core to further optimize our application.\n",
    "\n",
    "## Tensor Core Usage\n",
    "\n",
    "Tensor cores are specialized processing units designed to accelerate the process of tensor/matrix multiplication. Tensor Cores enable mixed-precision computing, dynamically adapting calculations to accelerate throughput while preserving accuracy. Our application runs on the `NVIDIA® DGX™ A100 Ampere architecture` GPU. You can also run the application on other GPU architectures, for example `NVIDIA Turing™ architecture` which has Tensor Core precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/tensor_cores.jpg height=\"60%\" width=\"60%\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screenshot below shows a table of NVIDIA GPU architectures and supported Tensor Core precisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/architecture_tensor_cores.jpg height=\"70%\" width=\"70%\"> </center>\n",
    "<center><a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\"> Source: NVIDIA website</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Profile Report\n",
    "To verify if the application uses Tensor Cores, we will use a new feature in NVIDIA Nsight™ Systems: **GPU performance metrics sampling**. Notice in the previous notebook, to profile the application after the second optimization we used the Nsight Systems `--gpu-metrics-device=all` CLI option. This enables the collection of the new feature and is intended to measure the utilization of different GPU subsystems. Hardware counters within the GPU are periodically read and used to generate performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the application's Tensor Cores usage by examining the report `(secondOptimization.nsys-rep)` in the Nsight Systems GUI. Scroll down to the bottom of the timeline until you see the rows for GPU metrics. Expand the `SM instructions` timeline row to see the `Tensor Active` which represents the ratio of `cycles the SM tensor pipes or FP16x2 pipes were active issuing tensor instructions` to `the number of cycles in the sample period` as a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/report_activate_tensor.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the screenshot above, the percentage graph is an `average of 5.7%` and `maximum of 45%`, so the application already uses the Tensor Cores on the A100 GPU. But, this is not the case for other architectures. For example, after examining the secondOptimization.nsys-rep from NVIDIA Turing™ GPU architecture, the percentage graph is zero at `Tensor Active/FP16 Activate`. Therefore, Tensor Core utilization has to be explicitly enabled using, for example, `automatic mixed precision (AMP)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=images/TensorCoreUsage.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision (AMP)\n",
    "\n",
    "Mixed Precision is the combined use of different numerical format `(single and half-precision computation)` in the training of a deep neural network.\n",
    "- single precision: FP32\n",
    "- half precision: FP16\n",
    "\n",
    "The use of mixed precision is possible in NVIDIA GPU architectures such as `Ampere`, `Volta™` , and `Turing`. The benefits include:\n",
    "\n",
    "- speed up of math-intensive operations using tensor cores,\n",
    "- require less memory bandwidth, thus data transfer operations are speedup, and\n",
    "- require less memory thus, the training and deployment of larger neural networks are possible.\n",
    "\n",
    "AMP automates the process of training using mixed precision through deep neural network (DNN) frameworks. PyTorch has the [Automatic Mixed Precision (AMP)](https://pytorch.org/docs/stable/amp.html) package which provides a simple way for users to convert existing FP32 training scripts to mixed FP32 and FP16 precision. This unlocks faster computation with Tensor Cores on NVIDIA GPUs. In the screenshot below you will see the code changes made `(in green color frame)` to use the AMP package in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/amp.jpg height=\"50%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile again and verify the code change addresses Tensor Core usage on the Turing GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nsys profile --trace cuda,osrt,nvtx \\\n",
    "--capture-range cudaProfilerApi \\\n",
    "--gpu-metrics-device=all \\\n",
    "--output ../reports/thirdOptimization_env \\\n",
    "--force-overwrite true \\\n",
    "python3 ../source_code/main_opt3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the report (thirdOptimization.nsys-rep) in the GUI. Scroll down to view the `Tensor Active / FP16 Active` timeline row.\n",
    "\n",
    "<img src=images/Optimization3.jpg>\n",
    "\n",
    "Now, we can see the Tensor Cores usage on the Turing GPU. Note that the main contribution of AMP is that it reduces the kernel time using Tensor Cores thereby achieving a speedup.\n",
    "\n",
    "## Compare the Performance Before and After the Optimizations\n",
    "Now that we have addressed three different performance problems, we will time the application [main_opt3.py](../source_code/main_opt3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && time python3 main_opt3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output on A100 GPUs**:\n",
    "\n",
    "```python\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308961\n",
    "\n",
    "Test set: Average loss: 0.1024, Accuracy: 9683/10000 (97%)\n",
    "\n",
    "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.154755\n",
    "\n",
    "Test set: Average loss: 0.0608, Accuracy: 9814/10000 (98%)\n",
    "\n",
    "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.110753\n",
    "\n",
    "Test set: Average loss: 0.0535, Accuracy: 9827/10000 (98%)\n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.059646\n",
    "\n",
    "Test set: Average loss: 0.0370, Accuracy: 9865/10000 (99%)\n",
    "\n",
    "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.055018\n",
    "\n",
    "Test set: Average loss: 0.0365, Accuracy: 9865/10000 (99%)\n",
    "\n",
    "\n",
    "real\t1m24.619s\n",
    "user\t2m35.069s\n",
    "sys\t 0m7.676s\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the time taken to run our baseline code [main_baseline.py](../source_code/main_baseline_nvtx.py) from [notebook 2](02_pytorch_mnist.ipynb) with the code after applying the three recent optimizations so far [main_opt3.py](../source_code/main_opt3.py), we see that the overall time taken has reduced as shown in the table below.\n",
    "\n",
    "\n",
    "|Training code| Time|speedup|\n",
    "|--|--|--|\n",
    "|basline| 113s|-|\n",
    "|optimized|~85|1.3x|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA Nsight Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download the latest version of NVIDIA Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "\n",
    "You can also get resources from [Open Hackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"05_summary.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
