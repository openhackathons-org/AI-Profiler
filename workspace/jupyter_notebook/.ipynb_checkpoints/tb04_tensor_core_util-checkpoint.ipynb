{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"tb05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"tb05_summary.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Tensor Cores \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show how to enable mixed precision on the Tensor Core to further optimize our application.\n",
    "\n",
    "## Tensor Core Usage\n",
    "\n",
    "Tensor cores are specialized processing units designed to accelerate the process of tensor/matrix multiplication. Tensor Cores enable mixed-precision computing, dynamically adapting calculations to accelerate throughput while preserving accuracy. Our application runs on the `NVIDIA® DGX™ A100`  Tensor Core GPU. You can also run the application on other GPU architectures, for example, the `Turing™ GPU architecture` which has Tensor Core precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/tensor_cores.jpg height=\"60%\" width=\"60%\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screenshot below shows a table of NVIDIA GPU architectures and supported Tensor Core precisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/architecture_tensor_cores.jpg height=\"70%\" width=\"70%\"> </center>\n",
    "<center><a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\"> Source here</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the TensorBoard Visualization\n",
    "\n",
    "To verify Tensor Core usage, check the `GPU Summary` frame. You can also verify through the `Kernel View` by selecting `GPU Kernel` from the Views dropdown menu. As shown below, the `Kernel Time using Tensor Cores` is `30.7%` and likewise the `Tensor Core Utilization` in the `Kernel View`.  We are able to see this because Tensor Core utilization is automatic with Ampere architecture-based GPUs such as those within the DGX A100. However, this may not be the case if you are running the lab on other GPU architectures. Our aim is to introduce `Automatic Mixed Precision (AMP)` Tensor Core operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/profile_summary_opt22_tensor_core.jpg>\n",
    "<img src=images/tensor_core_util_opt22.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision (AMP)\n",
    "\n",
    "Mixed Precision is the combined use of different numerical formats `(single and half-precision computation)` in the training of a deep neural network.\n",
    "- Single precision: FP32\n",
    "- Half precision: FP16\n",
    "\n",
    "The use of mixed precision is possible in NVIDIA GPU architectures such as `Ampere`, `Volta™`, and `Turing`. The benefits include:\n",
    "\n",
    "- Speed up math-intensive operations using tensor cores.\n",
    "- Requires less memory bandwidth, thus achieving a speedup of data transfer operations. \n",
    "- Requires less memory enabling the training and deployment of larger neural networks.\n",
    "\n",
    "\n",
    "Automatic Mixed Precision (AMP) automates the process of training using mixed precision through DNN frameworks. PyTorch has [AMP](https://pytorch.org/docs/stable/amp.html) package which provides a simple way for users to convert existing FP32 training scripts to mixed FP32 and FP16 precision. This unlocks faster computation with Tensor Cores on NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we introduce `AMP` into our application code, our first step is to search for `fp16` operations running within the Tensor Core in the `Kernel View`. The result below  shows it is absent.\n",
    "\n",
    "<img src=images/fp16_opt22.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the screenshot below you will see the code changes (in green color frame) made to use the AMP package in PyTorch.\n",
    "\n",
    "<img src=images/amp.jpg height=\"50%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile again and verify the code change enables `fp16` operations within the Tensor Core operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ../source_code/tb_main_opt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the cell below to visualize the profile in the TensorBoard. If you are working on a remote machine, remember to do `port-forward` before opening the browser at `localhost:6006/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=../log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/fp16_opt33.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see `fp16` ops under the `Name` column and the Yes that validate operations running within the Tensor Core under the `Tensor Cores Used` column. The impact of this on our application model is that it reduces the amount of time spent by the Tensor Core for computation because `AMP` enabled the speedup of math-intensive operations. This is verified in the `GPU Summary` frame shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/profile_summary_opt33.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/step_time_breakdown_opt33.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/trace_view_opt33.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following changes were found after `AMP` was activated:\n",
    "\n",
    "- Increase in GPU usage from `37%` to `52%`,\n",
    "- Time spent by DataLoader was further reduced from `4,638µs`(~27.9%) to `948µs` (10.7%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Performance Before and After the Optimizations\n",
    "Now that three different performance problems have been addressed, let us time the application  [tb_main_opt3.py](../source_code/tb_main_opt3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && time python3 main_opt3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output on DGX A100**:\n",
    "\n",
    "```python\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308961\n",
    "\n",
    "Test set: Average loss: 0.1024, Accuracy: 9683/10000 (97%)\n",
    "\n",
    "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.154755\n",
    "\n",
    "Test set: Average loss: 0.0608, Accuracy: 9814/10000 (98%)\n",
    "\n",
    "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.110753\n",
    "\n",
    "Test set: Average loss: 0.0535, Accuracy: 9827/10000 (98%)\n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.059646\n",
    "\n",
    "Test set: Average loss: 0.0370, Accuracy: 9865/10000 (99%)\n",
    "\n",
    "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.055018\n",
    "\n",
    "Test set: Average loss: 0.0365, Accuracy: 9865/10000 (99%)\n",
    "\n",
    "\n",
    "real\t1m24.619s\n",
    "user\t2m35.069s\n",
    "sys\t 0m7.676s\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the time taken to run our baseline code [main_baseline.py](../source_code/tb_main_baseline_nvtx.py) from [notebook 2](tb02_pytorch_mnist.ipynb) with the code after applying the three optimizations [main_opt3.py](../source_code/main_opt3.py), we see that the overall time taken was reduced as shown in the table below.\n",
    "\n",
    "\n",
    "|Training code| Time|speedup\n",
    "|--|--|--|\n",
    "|basline| 113s|-|\n",
    "|optimized|~85s|1.3x|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Links and Resources\n",
    "\n",
    "[NVIDIA Nsight™ Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download the latest version of NVIDIA Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "You can also get resources from [openhackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    " \n",
    " ---\n",
    " ## Licensing\n",
    "  \n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"tb05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"tb05_summary.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
