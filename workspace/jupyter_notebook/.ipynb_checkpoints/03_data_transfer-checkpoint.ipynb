{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a>3</a>\n",
    "        <a href=\"04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"04_tensor_core_util.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Transfers Between Host (CPU) and Device (GPU)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to optimize data transfer between Host and Device. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Report\n",
    "\n",
    "Let's analyze the data transfers between host and graphics processing unit (GPU) in the report `firstOptimization.nsys-rep` from the first optimization step. Open the report in the NVIDIA® Nsight™ Systems graphical user interface (GUI). Expand the `NVIDIA CUDA® device row` by clicking on the tiny triangle in front of it. Select the `Memory` row and right-click to choose `Show in Events View` option as shown below.\n",
    "\n",
    "<img src=\"images/report_show_in_events_view.jpg\" height=\"30%\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This populates the `Events View` window with the memory operations listed in chronological order. Click on the `Duration` column header to sort the table in the Events View by duration so that the longest memory operation shows up first. Right-click on the first entry in the table and select \"Show Current on Timeline\" as illustrated below.\n",
    "\n",
    "<img src=images/report_show_current_timeline.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This zooms into the event on the timeline and the teal highlights help you find the CUDA API call, `cudaMemcpyAsync`, that initiated the memory operation on the GPU (see the image below). Note: You may have to zoom out and/or scroll up to find the CUDA API call on the CPU thread.\n",
    "\n",
    "<img src=images/report_api_call.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You notice the following from the timeline:\n",
    "- All Host-to-Device (HtoD) memcopies are using pageable memory which is:\n",
    "    - slower and, \n",
    "    - causes the `cudaMemcpyAsync` API call on the CPU thread to block until the operation completes on the GPU.\n",
    "- The longest memcpy operation takes ~385 microseconds to complete on the GPU.\n",
    "- The CUDA API call (`cudaMemcpyAsync`) corresponding to the longest memcpy operation is almost 0.5ms long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the Application to Use Pinned Memory\n",
    "\n",
    "Host (CPU) memory allocations are pageable by default. The GPU cannot access data directly from pageable host memory. When a data transfer is invoked from pageable host memory to device memory, the CUDA driver must first allocate a temporary page-locked (or “pinned”) host array, copy the host data to the pinned array, and then transfer the data from the pinned array to the device memory. The pinned memory is used as a staging area for transfers from the host to the device. By directly allocating our host data to pinned memory, we can avoid this extra step and its overhead. See the blog [post](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/) for more details.\n",
    "\n",
    "<img src=images/PageableVsPinned.jpg width=50%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The settings used for the data loader [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) in our application relies on the default value of `pin_memory: False`. Execute the cell below to see the code change made `(in green color)` to use pinned memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--- ../source_code/main_opt1.py\t2022-08-18 03:51:54.178624632 +0900\u001b[0m\n",
      "\u001b[1m+++ ../source_code/main_opt2.py\t2022-08-19 23:25:01.750840799 +0900\u001b[0m\n",
      "\u001b[36m@@ -160,8 +160,9 @@\u001b[0m\n",
      "     test_kwargs = {'batch_size': args.test_batch_size}\n",
      "     if use_cuda:\n",
      "         #multiprocessing.cpu_count()\n",
      "         cuda_kwargs = {'num_workers': 2,\n",
      "\u001b[32m+                       'pin_memory': True,\u001b[0m\n",
      "                        'shuffle': True}\n",
      "         train_kwargs.update(cuda_kwargs)\n",
      "         test_kwargs.update(cuda_kwargs)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "!diff -U4 --color=always ../source_code/main_opt1.py ../source_code/main_opt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Again to Verify Optimization\n",
    "Profile again by executing the cell given below to verify if the code change addresses the problem with host-to-device memory transfers after setting `pin_memory: True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nsys profile --trace cuda,osrt,nvtx \\\n",
    "--capture-range cudaProfilerApi \\\n",
    "--gpu-metrics-device=all \\\n",
    "--output ../reports/secondOptimization \\\n",
    "--force-overwrite true \\\n",
    "python3 ../source_code/main_opt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the report (secondOptimization.nsys-rep) in the GUI. Similar to how we navigated the timeline previously, expand the `CUDA device` row and select the `Memory` row and right-click to choose `Show in Events View`. Sort the table in the `Events View` by duration to list the longest memory operation first. Right-click on the topmost event to select `Show current on timeline`. You should see the view as shown below.\n",
    "\n",
    "<img src=\"images/report_pinned_memory.jpg\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the profile collected after optimization, we observed that:\n",
    "- All HtoD memcopies now use pinned memory,\n",
    "- The longest memcpy is now only `183µs` compared to`~385µs` before optimization, and\n",
    "- The `cudaMemcpyAsync` API call corresponding to the longest memcpy is now reduced from `490µs` to `~36µs`.\n",
    "\n",
    "Now that we have addressed a bottleneck with memory transfers, let's identify the next performance bottleneck by clicking on the `Next Notebook` link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA Nsight Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download the latest version of NVIDIA Nsight Systems from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "\n",
    "You can also get resources from [Open Hackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a >3</a>\n",
    "        <a href=\"04_tensor_core_util.ipynb\">4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"04_tensor_core_util.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
