{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: right; width: 49%; text-align: right;\"><a href=\"05_summary.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Tensor Cores \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is show how to enable mixed precision (FP32/FP16) on the Tensor Core to further optimize our application.\n",
    "\n",
    "## Tensor Core Usage\n",
    "\n",
    "Tensor cores are specialized processing units designed to accelerate the process of tensor/matrix multiplication. Tensor Cores enable mixed-precision computing, dynamically adapting calculations to accelerate throughput while preserving accuracy. Our application runs on the DGX A100 `Ampere architecture` GPU. You can also run the application on other GPUs, for example, `Turing GPU architecture` which has tensor Core precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/tensor_cores.jpg height=\"60%\" width=\"60%\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The screenshot below shows a table of GPU architecture and supported Tensor Cores precisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=images/architecture_tensor_cores.jpg height=\"70%\" width=\"70%\"> </center>\n",
    "<center><a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\"> view source here</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the profile report\n",
    "To verify if the application uses Tensor Cores, we will use a new feature in Nsight Systems: **GPU performance metrics sampling**. Notice that in the Nsight Systems command line used to profile the application after second optimization in previous notebook, we used the `--gpu-metrics-device=all` CLI option. This enables the collection of the new feature. It is intended to measure the utilization of different GPU subsystems. Hardware counters within the GPU are periodically read and used to generate performance metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the application's Tensor Cores usage by examining the report `(secondOptimization.nsys-rep)` in the Nsight Systems GUI. Scroll down to the bottom of the timeline until you see the rows for GPU metrics. Expand the `SM instructions` timeline row to see the `Tensor Active` which represents the ratio of *cycles the SM tensor pipes or FP16x2 pipes were active issuing tensor instructions* to *the number of cycles in the sample period* as a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/report_activate_tensor.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the screenshot above, the percentage graph is `average of 5.7%` and `maximum of 45%`, so the application already use the Tensor Cores on the DGX A100. But this is not the case for other architectures, for example, after examining the `secondOptimization.nsys-rep` from Turing GPU the percentage graph is `zero` at `Tensor Active/FP16 Activate`. Therefore, Tensor Core utilization has to be explicitly enabled using for example, the use of `Automatic Mixed Precision (AMP)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=images/TensorCoreUsage.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Mixed Precision (AMP)\n",
    "\n",
    "Mixed Precision is the combine use of different numerical format `(single and half precision computation)` in the training of a deep neural network.\n",
    "- single precision: FP32\n",
    "- half precision: FP16\n",
    "\n",
    "\n",
    "The use of Mixed precision is possible in GPU architectures such as `Ampere`, `Volta`, and `Turing`. The benefits include:\n",
    "- speed up of math-intensive operations using tensor cores\n",
    "- require less memory bandwidth, thus data transfer operations are speedup\n",
    "- require less memory thus, the training and deployment of larger neural networks are possible.\n",
    "\n",
    "Automatic Mixed Precision AMP automates the process of training using mixed precision through DNN frameworks. PyTorch has [Automatic Mixed Precision (AMP)](https://pytorch.org/docs/stable/amp.html) package which provides a simple way for users to convert existing FP32 training scripts to mixed FP32 & FP16 precision. This unlocks faster computation with Tensor Cores on NVIDIA GPUs. In the screenshot below you will see the code changes made `(in green color frame)` to use AMP package in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/amp.jpg height=\"50%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's profile again and verify if the code change addresses Tensor Core usage on Turing GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nsys profile --trace cuda,osrt,nvtx \\\n",
    "--capture-range cudaProfilerApi \\\n",
    "--gpu-metrics-device=all \\\n",
    "--output ../reports/thirdOptimization_env \\\n",
    "--force-overwrite true \\\n",
    "python3 ../source_code/main_opt3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the report (thirdOptimization.nsys-rep) in the GUI. Scroll down to view the `Tensor Active / FP16 Active` timeline row.\n",
    "\n",
    "<img src=images/Optimization3.jpg>\n",
    "\n",
    "Now, we can see the Tensor Cores usage on the Turing GPU. Note that the main contribution of AMP is that it reduces the kernel time using Tensor Cores, hence, speedup is achieved.\n",
    "\n",
    "## Compare the Performance Before and After the Optimizations\n",
    "Now that we have addressed three different performance problems, let's time the application [main_opt3.py](../source_code/main_opt3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ../source_code && time python3 main_opt3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output on DGX A100**:\n",
    "\n",
    "```python\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308961\n",
    "\n",
    "Test set: Average loss: 0.1024, Accuracy: 9683/10000 (97%)\n",
    "\n",
    "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.154755\n",
    "\n",
    "Test set: Average loss: 0.0608, Accuracy: 9814/10000 (98%)\n",
    "\n",
    "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.110753\n",
    "\n",
    "Test set: Average loss: 0.0535, Accuracy: 9827/10000 (98%)\n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.059646\n",
    "\n",
    "Test set: Average loss: 0.0370, Accuracy: 9865/10000 (99%)\n",
    "\n",
    "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.055018\n",
    "\n",
    "Test set: Average loss: 0.0365, Accuracy: 9865/10000 (99%)\n",
    "\n",
    "\n",
    "real\t1m24.619s\n",
    "user\t2m35.069s\n",
    "sys\t 0m7.676s\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the time taken to run our baseline code [main_baseline.py](../source_code/main_baseline_nvtx.py) from [notebook 2](02_pytorch_mnist.ipynb) with the code after applying the three optimizations so far [main_opt3.py](../source_code/main_opt3.py), we see that the overall time taken has reduced as shown in the table below.\n",
    "\n",
    "\n",
    "|Training code| Time|speedup|\n",
    "|--|--|--|\n",
    "|basline| 113s|-|\n",
    "|optimized|~85|1.3x|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and Resources\n",
    "\n",
    "\n",
    "[NVIDIA® Nsight™ Systems](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "\n",
    "**NOTE**: To be able to see the profiler output, please download NVIDIA Nsight Systems' latest version from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "You can also get resources from [openhackathons technical resource page](https://www.openhackathons.org/s/technical-resources)\n",
    "\n",
    "---\n",
    " ## Licensing\n",
    "  \n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 51%; text-align: right;\">\n",
    "        <a href=\"01_introduction.ipynb\">1</a>\n",
    "        <a href=\"02_pytorch_mnist.ipynb\">2</a>\n",
    "        <a href=\"03_data_transfer.ipynb\">3</a>\n",
    "        <a >4</a>\n",
    "        <a href=\"05_summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 49%; text-align: right;\"><a href=\"05_summary.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<p> <center> <a href=\"../start_here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
